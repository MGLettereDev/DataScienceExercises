{"cells":[{"cell_type":"markdown","metadata":{"id":"qG-_vGMEahkC"},"source":["### Apellidos y Nombres:"]},{"cell_type":"markdown","metadata":{"id":"7RiF3D63ahkJ"},"source":["Lettere Dragosavljevich Mathias Giuseppe"]},{"cell_type":"markdown","metadata":{"id":"S1u1NwOFahkK"},"source":["### Fecha:"]},{"cell_type":"markdown","metadata":{"id":"sOoEXfgpahkL"},"source":["05-09-2023"]},{"cell_type":"markdown","metadata":{"id":"NhblZDjQahkL"},"source":["# **Introduction to SparkSQL**\n"]},{"cell_type":"markdown","metadata":{"id":"RMjn__hvahkM"},"source":["This lab goes over the basic operations of Apache SparkSQL.\n"]},{"cell_type":"markdown","metadata":{"id":"h44oHl0kahkP"},"source":["***\n"]},{"cell_type":"markdown","metadata":{"id":"CHtVbreJahkQ"},"source":["## Google Colab Setup"]},{"cell_type":"markdown","metadata":{"id":"Dl03JOtLahkR"},"source":["If you are going to use Google Colab instead of a Spark Cluster, you will need to run the following code to install Apache Spark."]},{"cell_type":"code","execution_count":22,"metadata":{"id":"e2XYx3KgahkR","executionInfo":{"status":"ok","timestamp":1693920199694,"user_tz":300,"elapsed":8373,"user":{"displayName":"Mathias Lettere","userId":"17227849843144031062"}}},"outputs":[],"source":["!apt-get install openjdk-8-jdk-headless -qq > /dev/null"]},{"cell_type":"code","execution_count":23,"metadata":{"id":"axA2LxlvahkU","executionInfo":{"status":"ok","timestamp":1693920223767,"user_tz":300,"elapsed":13205,"user":{"displayName":"Mathias Lettere","userId":"17227849843144031062"}}},"outputs":[],"source":["#If the following links don't work, you will have to update them with the last versions of Apache Spark\n","!wget -q https://dlcdn.apache.org/spark/spark-3.4.1/spark-3.4.1-bin-hadoop3.tgz\n","!tar xf spark-3.4.1-bin-hadoop3.tgz"]},{"cell_type":"code","execution_count":24,"metadata":{"id":"0VVmflyKahkU","executionInfo":{"status":"ok","timestamp":1693920278026,"user_tz":300,"elapsed":501,"user":{"displayName":"Mathias Lettere","userId":"17227849843144031062"}}},"outputs":[],"source":["import os\n","os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n","os.environ[\"SPARK_HOME\"] = \"/content/spark-3.4.1-bin-hadoop3\""]},{"cell_type":"markdown","metadata":{"id":"q0BEq40gahkV"},"source":["## Setup\n"]},{"cell_type":"code","execution_count":25,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XJv1xN95ahkV","executionInfo":{"status":"ok","timestamp":1693920308280,"user_tz":300,"elapsed":27497,"user":{"displayName":"Mathias Lettere","userId":"17227849843144031062"}},"outputId":"0a961443-bccd-4b9d-bcee-a6834cd14577"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: pyspark in /usr/local/lib/python3.10/dist-packages (3.4.1)\n","Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n","Requirement already satisfied: findspark in /usr/local/lib/python3.10/dist-packages (2.0.1)\n","Requirement already satisfied: pyarrow in /usr/local/lib/python3.10/dist-packages (9.0.0)\n","Requirement already satisfied: numpy>=1.16.6 in /usr/local/lib/python3.10/dist-packages (from pyarrow) (1.23.5)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (1.5.3)\n","Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2023.3)\n","Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.23.5)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.23.5)\n","\u001b[31mERROR: Could not find a version that satisfies the requirement random (from versions: none)\u001b[0m\u001b[31m\n","\u001b[0m\u001b[31mERROR: No matching distribution found for random\u001b[0m\u001b[31m\n","\u001b[0m"]}],"source":["# Installing required packages\n","!pip install pyspark\n","!pip install findspark\n","!pip install pyarrow\n","!pip install pandas\n","!pip install numpy\n","!pip install random"]},{"cell_type":"code","source":[],"metadata":{"id":"ADUn_VznWmWf"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":26,"metadata":{"id":"7Bqaf7bSahkW","executionInfo":{"status":"ok","timestamp":1693920312126,"user_tz":300,"elapsed":246,"user":{"displayName":"Mathias Lettere","userId":"17227849843144031062"}}},"outputs":[],"source":["import findspark\n","findspark.init()"]},{"cell_type":"code","execution_count":27,"metadata":{"id":"3ldZgwQMahkW","executionInfo":{"status":"ok","timestamp":1693920313237,"user_tz":300,"elapsed":2,"user":{"displayName":"Mathias Lettere","userId":"17227849843144031062"}}},"outputs":[],"source":["import pandas as pd\n","from pyspark import SparkContext, SparkConf\n","from pyspark.sql import SparkSession"]},{"cell_type":"code","source":["import random"],"metadata":{"id":"aNK_yUfZYNjM","executionInfo":{"status":"ok","timestamp":1693920315471,"user_tz":300,"elapsed":298,"user":{"displayName":"Mathias Lettere","userId":"17227849843144031062"}}},"execution_count":28,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"I8O1TL9SahkX"},"source":["## Spark session\n"]},{"cell_type":"markdown","metadata":{"id":"WPNN8FYXahkX"},"source":["#### Spark session and context\n"]},{"cell_type":"code","execution_count":29,"metadata":{"id":"qLydcaQBahkY","executionInfo":{"status":"error","timestamp":1693920318441,"user_tz":300,"elapsed":390,"user":{"displayName":"Mathias Lettere","userId":"17227849843144031062"}},"colab":{"base_uri":"https://localhost:8080/","height":286},"outputId":"94775910-5977-4032-eee6-8a9fd9cab552"},"outputs":[{"output_type":"error","ename":"ValueError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-29-bbecaf5a89c4>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Creating a spark context class\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0msc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Creating a spark session\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mspark\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkSession\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/spark-3.4.1-bin-hadoop3/python/pyspark/context.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls, udf_profiler_cls, memory_profiler_cls)\u001b[0m\n\u001b[1;32m    196\u001b[0m             )\n\u001b[1;32m    197\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 198\u001b[0;31m         \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgateway\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    199\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m             self._do_init(\n","\u001b[0;32m/content/spark-3.4.1-bin-hadoop3/python/pyspark/context.py\u001b[0m in \u001b[0;36m_ensure_initialized\u001b[0;34m(cls, instance, gateway, conf)\u001b[0m\n\u001b[1;32m    443\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    444\u001b[0m                     \u001b[0;31m# Raise error if there is already a running Spark context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 445\u001b[0;31m                     raise ValueError(\n\u001b[0m\u001b[1;32m    446\u001b[0m                         \u001b[0;34m\"Cannot run multiple SparkContexts at once; \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    447\u001b[0m                         \u001b[0;34m\"existing SparkContext(app=%s, master=%s)\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: Cannot run multiple SparkContexts at once; existing SparkContext(app=pyspark-shell, master=local[*]) created by __init__ at <ipython-input-7-bbecaf5a89c4>:2 "]}],"source":["# Creating a spark context class\n","sc = SparkContext()\n","\n","# Creating a spark session\n","spark = SparkSession \\\n","    .builder \\\n","    .appName(\"Python Spark DataFrames basic example\") \\\n","    .config(\"spark.some.config.option\", \"some-value\") \\\n","    .getOrCreate()"]},{"cell_type":"markdown","metadata":{"id":"MrM2sPb6ahkY"},"source":["#### Initialize Spark session\n","\n"]},{"cell_type":"code","execution_count":30,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":219},"id":"CXHKA9TmahkY","executionInfo":{"status":"ok","timestamp":1693920331856,"user_tz":300,"elapsed":677,"user":{"displayName":"Mathias Lettere","userId":"17227849843144031062"}},"outputId":"845be4a8-14e9-4636-9483-260c2bab172a"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["<pyspark.sql.session.SparkSession at 0x7ef165c751e0>"],"text/html":["\n","            <div>\n","                <p><b>SparkSession - in-memory</b></p>\n","                \n","        <div>\n","            <p><b>SparkContext</b></p>\n","\n","            <p><a href=\"http://3a45bf55fa57:4040\">Spark UI</a></p>\n","\n","            <dl>\n","              <dt>Version</dt>\n","                <dd><code>v3.4.1</code></dd>\n","              <dt>Master</dt>\n","                <dd><code>local[*]</code></dd>\n","              <dt>AppName</dt>\n","                <dd><code>pyspark-shell</code></dd>\n","            </dl>\n","        </div>\n","        \n","            </div>\n","        "]},"metadata":{},"execution_count":30}],"source":["spark"]},{"cell_type":"markdown","metadata":{"id":"E9Le2R3CahkZ"},"source":["## Media\n"]},{"cell_type":"code","execution_count":31,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"C5xuS56mahka","executionInfo":{"status":"ok","timestamp":1693920341237,"user_tz":300,"elapsed":1490,"user":{"displayName":"Mathias Lettere","userId":"17227849843144031062"}},"outputId":"735468db-bfb1-4ac8-e2b3-f60b89c4242e"},"outputs":[{"output_type":"stream","name":"stdout","text":["La madia del RDD es:  164.86\n"]}],"source":["from pyspark.rdd import RDD\n","\n","rdd = sc.parallelize(random.sample(range(5, 300), 100))\n","\n","def media(d):\n","  sum = d.sum()\n","  n = d.count()\n","  return sum/float(n)\n","\n","print(\"La madia del RDD es: \", media(rdd))"]},{"cell_type":"markdown","metadata":{"id":"DClvIgjwahkb"},"source":["## Mediana\n"]},{"cell_type":"code","source":["def mediana(d):\n","  sortear = d.sortBy(lambda x:x)\n","  indexar = sortear.zipWithIndex().map(lambda value_key : (value_key[1], value_key[0]))\n","  n = d.count()\n","\n","  if(n%2 == 1):\n","    MIndex = (n-1)/2\n","    return indexar.filter(lambda key_value : key_value[0] == MIndex).collect()[0][1]\n","  else:\n","    i1 = (n/2)-1\n","    i2 = n/2\n","\n","    v1 = indexar.filter(lambda key_value : key_value[0] == i1).collect()[0][1]\n","    v2 = indexar.filter(lambda key_value : key_value[0] == i2).collect()[0][1]\n","    return (v1+v2)/2\n","\n","print(\"La mediana del RDD es: \", mediana(rdd))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"X21EP5kIa4ae","executionInfo":{"status":"ok","timestamp":1693920560635,"user_tz":300,"elapsed":2545,"user":{"displayName":"Mathias Lettere","userId":"17227849843144031062"}},"outputId":"c96d5af4-b9c9-44a2-cb19-3049a435e1cc"},"execution_count":37,"outputs":[{"output_type":"stream","name":"stdout","text":["La mediana del RDD es:  176.0\n"]}]},{"cell_type":"markdown","source":["## Desviación Estándar"],"metadata":{"id":"dBiobC8Ldru0"}},{"cell_type":"code","source":["from math import sqrt\n","def stdDev(d):\n","  m = media(d)\n","  n = d.count()\n","  return sqrt(d.map(lambda x: pow(x - m, 2)).sum()/float(n))\n","\n","#m = sqrt(rdd.map(lambda x: pow(x - media(d), 2).sum()/float(rdd.count))\n","print(\"La desviación estándar del RDD es: \", stdDev(rdd))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PJcV7yzmd2pF","executionInfo":{"status":"ok","timestamp":1693920932359,"user_tz":300,"elapsed":1326,"user":{"displayName":"Mathias Lettere","userId":"17227849843144031062"}},"outputId":"f1d42580-3a1d-4664-bbe6-784428dcfae3"},"execution_count":44,"outputs":[{"output_type":"stream","name":"stdout","text":["La desviación estándar del RDD es:  83.20505032748915\n"]}]},{"cell_type":"markdown","source":["## Skewness"],"metadata":{"id":"oxWLcj9xjO4l"}},{"cell_type":"code","source":["def skw(d):\n","  m = media(d)\n","  sd = stdDev(d)\n","  n = d.count()\n","  return (1/n)*d.map(lambda x: pow(x-m, 3)/pow(sd,3)).sum()\n","\n","print(\"La Skewdness del RDD es: \", skw(rdd))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IVEbvCwcjSpQ","executionInfo":{"status":"ok","timestamp":1693921142768,"user_tz":300,"elapsed":3940,"user":{"displayName":"Mathias Lettere","userId":"17227849843144031062"}},"outputId":"c7e745d0-444f-4376-b041-5dee01d3cbf5"},"execution_count":45,"outputs":[{"output_type":"stream","name":"stdout","text":["La Skewdness del RDD es:  -0.2859177847201621\n"]}]},{"cell_type":"markdown","source":["## Kurtosis"],"metadata":{"id":"lJir1myzjTAY"}},{"cell_type":"code","source":["def kurt(d):\n","  m = media(d)\n","  sd = stdDev(d)\n","  n = d.count()\n","  return (1/n)*d.map(lambda x: pow(x-m, 4)/pow(sd,4)).sum()\n","\n","print(\"La Kurtosis del RDD es: \", kurt(rdd))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"s0vF0pJ4kXtw","executionInfo":{"status":"ok","timestamp":1693921200132,"user_tz":300,"elapsed":2515,"user":{"displayName":"Mathias Lettere","userId":"17227849843144031062"}},"outputId":"5fba3160-6fd0-48a4-a513-246f4bd2a50a"},"execution_count":46,"outputs":[{"output_type":"stream","name":"stdout","text":["La Kurtosis del RDD es:  1.9711548070784601\n"]}]},{"cell_type":"markdown","source":["## Covarianza"],"metadata":{"id":"sv0ZIp_fjVGf"}},{"cell_type":"code","source":["rdd2 = sc.parallelize(random.sample(range(5, 300), 100))\n","\n","def cov(x, y):\n","  rddXY = x.zip(y)\n","  mx= media(x)\n","  my= media(y)\n","  return rddXY.map(lambda x_y: (x_y[0] - mx)*(x_y[1]-my)).sum()/rddXY.count()\n","\n","print(\"La Covarianza entre X y Y es: \", cov(rdd, rdd2))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"79m7alzQkuF9","executionInfo":{"status":"ok","timestamp":1693921534848,"user_tz":300,"elapsed":1907,"user":{"displayName":"Mathias Lettere","userId":"17227849843144031062"}},"outputId":"a912ca4f-eaf4-4d09-ee79-98739d8c8a67"},"execution_count":47,"outputs":[{"output_type":"stream","name":"stdout","text":["La Covarianza entre X y Y es:  1396.9624\n"]}]},{"cell_type":"markdown","source":["## Correlación"],"metadata":{"id":"bo4R_glbjYzw"}},{"cell_type":"code","source":["def correl(x, y):\n","  covXY = cov(x, y)\n","  sdx = stdDev(x)\n","  sdy = stdDev(y)\n","  return covXY/(sdx*sdy)\n","\n","print(\"La Correlación entre X y Y es: \", correl(rdd, rdd2))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Ei9JmcJ1mD-R","executionInfo":{"status":"ok","timestamp":1693921784689,"user_tz":300,"elapsed":4891,"user":{"displayName":"Mathias Lettere","userId":"17227849843144031062"}},"outputId":"3dacd877-635c-40a8-f617-8e9148174488"},"execution_count":49,"outputs":[{"output_type":"stream","name":"stdout","text":["La Correlación entre X y Y es:  0.2035579950399146\n"]}]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.7"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}